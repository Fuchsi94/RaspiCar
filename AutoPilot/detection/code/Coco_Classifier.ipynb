{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Coco_Classifier.ipynb","provenance":[],"authorship_tag":"ABX9TyObjgJWNEClkzH6C01P1jrH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":368},"id":"a0gLrTGl2PwQ","executionInfo":{"status":"error","timestamp":1630055006389,"user_tz":-120,"elapsed":281,"user":{"displayName":"Yannick Fuchs","photoUrl":"","userId":"07953827325314486284"}},"outputId":"93c6f0e5-2480-46c4-9e16-f36f1f553607"},"source":["\"\"\"A demo to classify Raspberry Pi camera stream.\"\"\"\n","import argparse\n","import time\n","\n","import numpy as np\n","import os\n","import datetime\n","\n","import edgetpu.detection.engine\n","import cv2\n","from PIL import Image\n","\n","def main():\n","    os.chdir('/home/pi/DeepPiCar/models/object_detection')\n","    \n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\n","      '--model', help='File path of Tflite model.', required=False)\n","    parser.add_argument(\n","      '--label', help='File path of label file.', required=False)\n","    args = parser.parse_args()\n","    \n","    args.model = 'data/model_result/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite'\n","    args.label = 'data/model_result/coco_labels.txt'\n","        \n","    with open(args.label, 'r') as f:\n","        pairs = (l.strip().split(maxsplit=1) for l in f.readlines())\n","        labels = dict((int(k), v) for k, v in pairs)\n","\n","    # initialize open cv\n","    IM_WIDTH = 640\n","    IM_HEIGHT = 480\n","    camera = cv2.VideoCapture(0)\n","    ret = camera.set(3,IM_WIDTH)\n","    ret = camera.set(4,IM_HEIGHT)\n","    \n","    font = cv2.FONT_HERSHEY_SIMPLEX\n","    bottomLeftCornerOfText = (10,IM_HEIGHT-10)\n","    fontScale = 1\n","    fontColor = (255,255,255)  # white\n","    boxColor = (0,0,255)   # RED?\n","    boxLineWidth = 1\n","    lineType = 2\n","    \n","    annotate_text = \"\"\n","    annotate_text_time = time.time()\n","    time_to_show_prediction = 1.0 # ms\n","    min_confidence = 0.20\n","    \n","    # initial classification engine\n","    engine = edgetpu.detection.engine.DetectionEngine(args.model)\n","    elapsed_ms = 0\n","    \n","    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","    out = cv2.VideoWriter('output.avi',fourcc, 20.0, (IM_WIDTH,IM_HEIGHT))\n","    \n","    \n","    try:\n","        while camera.isOpened():\n","            try:\n","                start_ms = time.time()\n","                ret, frame = camera.read() # grab a frame from camera\n","                if ret == False :\n","                    print('can NOT read from camera')\n","                    break\n","                \n","                frame_expanded = np.expand_dims(frame, axis=0)\n","                \n","                ret, img = camera.read()\n","                input = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # convert to RGB color space\n","                img_pil = Image.fromarray(input)\n","                #input = cv2.resize(input, (width,height))\n","                start_tf_ms = time.time()\n","                results = engine.DetectWithImage(img_pil, threshold=min_confidence, keep_aspect_ratio=True,\n","                                   relative_coord=False, top_k=5)\n","                end_tf_ms = time.time()\n","                elapsed_tf_ms = end_tf_ms - start_ms\n","                \n","                if results :\n","                    for obj in results:\n","                        \n","                        print(\"%s, %.0f%% %s %.2fms\" % (labels[obj.label_id], obj.score *100, obj.bounding_box, elapsed_tf_ms * 1000))\n","                        box = obj.bounding_box\n","                        coord_top_left = (int(box[0][0]), int(box[0][1]))\n","                        coord_bottom_right = (int(box[1][0]), int(box[1][1]))\n","                        cv2.rectangle(img, coord_top_left, coord_bottom_right, boxColor, boxLineWidth)\n","                        annotate_text = \"%s, %.0f%%\" % (labels[obj.label_id], obj.score * 100)\n","                        coord_top_left = (coord_top_left[0],coord_top_left[1]+15)\n","                        cv2.putText(img, annotate_text, coord_top_left, font, fontScale, boxColor, lineType )\n","                    print('------')\n","                else:\n","                    print('No object detected')\n","\n","                # Print Frame rate info\n","                elapsed_ms = time.time() - start_ms\n","                annotate_text = \"%.2f FPS, %.2fms total, %.2fms in tf \" % (1.0 / elapsed_ms, elapsed_ms*1000, elapsed_tf_ms*1000)\n","                print('%s: %s' % (datetime.datetime.now(), annotate_text))\n","                cv2.putText(img, annotate_text, bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n","                \n","                out.write(img)\n","                    \n","                cv2.imshow('Detected Objects', img)\n","                if cv2.waitKey(1) & 0xFF == ord('q'):\n","                    break\n","            except:\n","                # catch it and don't exit the while loop\n","                print('In except')\n","                traceback.print_exc()\n","\n","    finally:\n","        print('In Finally')\n","        camera.release()\n","        out.release()\n","        cv2.destroyAllWindows()\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7ce4fff455bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0medgetpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'edgetpu'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"GEv_Fned2bGp","executionInfo":{"status":"ok","timestamp":1630055031680,"user_tz":-120,"elapsed":1437,"user":{"displayName":"Yannick Fuchs","photoUrl":"","userId":"07953827325314486284"}},"outputId":"3ae86918-3031-4f24-eace-58e7d830dfbf","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install edgetpu"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement edgetpu (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for edgetpu\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JqKFVnqX2dyb"},"source":[""],"execution_count":null,"outputs":[]}]}